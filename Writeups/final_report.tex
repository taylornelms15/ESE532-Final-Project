\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project Final Report - Group}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle

\section{Introduction}
The elephant in the room for the entirety of our implementation is that, as of 24 hours before the due date of this project, it is not functioning correctly.
It gets through the entirely of the input, and (finally) does not hang in the middle of a deadlock or infinite loop somewhere in hardware, but upon encoding and decoding some files (particuarly, long binary files), the decoded version does not match the original.
\newline\newline
Now, that does not mean we cannot obtain reasonable ideas as to performance, area/time costs, etc. for our design. However, concerns such as specific design axes eluded us through development because we were unable to reach the essential milestone of "a working design."
\newline\newline
This is all to say that, while we cannot empirically verify any results of design changes, or give good accounts of verification strategies, what we \textbf{can} do is evaluate design decisions we could have made through the process, and analyze what kind of effects they would have on a mythical end result where our software worked.
\newline\newline
As such, you must excuse us if we are light on some empiric details, as we, even in this final stretch, endeavor to improve our design to the point of functionality. 90-minute build times are, unfortunately, not condusive to rapid prototyping, so we will do what we can to represent our control of the source material in an academic sense, if not always a practical one.

\section{Single ARM processor design}



\section{Ultra 96 design}
– Performance achieved and energy required
– Compression achieved
– Key design aspects: task decomposition, parallelism, mapping to Zynq resources, include diagrams to support
– Be clear where each component of the final design is performed (e.g., ARM, NEON vector, FPGA logic).
– Model to explain performance, area, and energy of design
– Current bottleneck preventing higher performance

All the components CDC, SHA, LZW and deduplication operate in hardware. 
Input data is read from network and it is processed in 2MB parts. 
// TODO: Detailed description about network reads
\newline\newline

Processor components:
\newline
Input data is read from network and it is processed in 2MB parts. 
\newline
The input read over the network is handled using a pthread that is executing on its own core. 
\newline
The primary processor core(core 0) is running the PS section that interfaces with the PL logic. 
\newline
The network thread is bound to run only on core 2. 
\newline

Network thread functionality: 
\newline
The thread uses a 200MB input buffer that is shared between the cores 2 and 0.
\newline
The NW thread then reads packets, calculates the length of each packet and then copies over the bytes to the buffer.
\newline
In our implementation, we use 2MB buffers to feed the dataflow model implemented on the FPGA.
\newline
So the NW thread has the additional responsibility to increment a counter everytime it crosses the 2MB boundary. 
This variable is checked against in the main processor to make certain that it does not try to process any data before the buffer data fills up. The variable is used as a synchronization mechanism.
\newline
Finally upon exit, the NW thread indicates that it has completed its purpose and marks a variable that is used by core 0 to figure out that no more packets are to be expected.

CDC performs chunking and the data is taken by SHA and LZW which operate parallely. The output of SHA which is 256 bit SHA value and the compressed output of LZW are both taken by deduplicate section to finally write the appropriate output to the output file. 
3 ARM cores are being used. One is used for reading the input data, one for processing the data and the third for writing the output to the output file.  

\subsection{Bottleneck}

Thanks to our decision to leverage the \texttt{DATAFLOW} pragma to handle our data movement in the PL, we have exactly zero idea of what the bottleneck in our design is.
\par
I know, that's not a great answer. When we try to make traces of our program execution, no tools that xilinx provides us allow for any kind of per-element trace of execution, or logs of streams being full/empty, or any other kind of relevant information. The best we have to go on are estimates of latencies of our functions, but, given that some of them run across variable-length chunks a variable number of times within our hardware buffer, the ranges are huge, and not reflective of actual execution.
\newline\par
However, what we \textbf{can} do is make estimates of what the bottlenecks might be. For instance: with a hardware buffer a couple factors of scale past our largest chunk workload, we can estimate that the data movement to and from the FPGA is not the primary concern; that overhead should amortize out far past any dominating bottleneck term.
\newline\par
So where, in our core logic inside the PL, could the bottleneck be? It is possible that it is the compute within the \texttt{SHA} function; there are certainly a lot of loops to perform for each block of $512$ bytes, and that necessarily takes time.
\par
Alternatively, the \texttt{LZW} unit could be the bottleneck. Though the inner function, which operates on a per-chunk basis, has been pipelined down to an \texttt{II} of $6$ (at 200MHz) or $9$ (at 300MHz), the overhead of clearing the validity bits for its hash table could be styming its ability to efficiently handle data at a high throughput.
\par
Our current bottleneck suspect is the access to shared memory for looking at the dictionary of \texttt{SHA} digests; this necessarily stalls the \texttt{deduplicate} unit until it can read a (current) total of $144$ bytes from shared memory for each chunk, which it then compares against our incoming \texttt{SHA} value. Depending on how efficiently this operation can work, this may hang up the entire operation. It is also possible that having to wait for this to be done before beginning to output our constructed packet to our final output stream could be holding back the design.
\par
The rest of our operations should be safe from bottlenecks; the reading of hardware buffer input into streams, the output from the final stream, and the process of Rabin-fingerprinting have all been pipelined effectively to initiation intervals of less than or equal to $2$.
\newline\par
Obviously, there are a lot of potential culprits, but the current front-runners are likely the \texttt{SHA} unit and the lookup section of the \texttt{Deduplicate} unit.

\section{10 Gbps design}

As has been noted, the fact that we are operating on a \texttt{DATAFLOW} model means that our ability to peek inside the specifics of our design is limited. Xilinx does not like putting that many (read: any) calipers on the internals of our hardware function.
\par
As such, we must examine possibilities for improvements in the abstract. In this section, we will discuss strategies for improving performance which, were we to work as diligently across the next $5$ weeks as we have for this previous $5$, would certainly yield performance improvements in pursuit of a $10Gbps$ goal.

\subsection{With Hardware Bounds in Mind}

Looking at our current design, we can approach designing a \texttt{10Gbps} design with the idea that the core functionality would still use logic almost entirely on the FPGA, but we could use more of our resources to solve the problem, and use as many tricks as possible to maximise the utility of the PL's time.
\newline\par
One of the first things we can see from our current resource usage is that we have a good number of BRAM's still available, as well as a large proportion of our FF/LUT resources.
This opens us up to putting more SHA units (for the computational resources) and LZW units (for the memory resources) onto the PL, with the idea that one of the two (likely the former) is the bottleneck culprit.
\par
With the idea that a singular \texttt{deduplicate} unit would still need to control output, we could link the \texttt{rabin} unit to the \texttt{deduplicate} unit by way of multiple parallel \texttt{SHA} units; for this example, we will imagine $2$ such units. This would allow us to alternate which unit hashed which series of chunks, allowing for an idealized $2x$ speedup on that axis.
\par
As for the \texttt{LZW} unit, we could easily fit another onto the PL in the same fashion.
\newline\par
If the resources are not the bottleneck, then we would need to look into the process by which we are transferring data. the use of \texttt{hls::stream} FIFO units has been handy for automatically handling a lot of timing requirements, but they carry the disadvantage of limiting the rate by which we are (conceptually) pipelining data through our application.
\par
One way we could improve upon this would be to use multiple streams in parallel for our application; conceptually, this would be similar to, instead of sending water through a single garden hose, sending water through two parallel garden hoses.
\par
If our bottleneck in the PL is access to the shared memory inside which our SHA dictionary resides, we could mitigate that problem by decreasing the amount of memory we need to read. We could do this by expanding our shared memory hash table region; if we're expecting something along the lines of $2^{16}$ unique chunks in a file, we could allocate $2^{19}$, or more, total hash lines such that we would probabilistically only need to pull one hash line for any given SHA digest.
\par
In this way, our total memory access would be in the range of $36$ bytes per chunk, rather than our current $144$. The disadvantage would be, we may need a particularly significant field of hash values to near-guarantee that we would not encounter any hash collisions that may break our design.
\newline\par
On our software side, we are currently using one thread/core purely to read in input, one to write to our output memory, and then one thread is copying memory to the FPGA working region, invoking the PL function, and then copying that memory out to a location from which the final thread writes.
\newline\par
A different implementation would more cleverly synchronize the operations that the middle thread performs, and make sure that the relevant memory copying did not need to happen (or at least, did not need to happen in a redundant fashion.)

\subsection{With No Hardware Bounds in Mind}

If we distance ourselves from the bounds of the Ultra96 system, however, we can consider all manner of other design options.
\par
Let us approach this by keeping the core of our design (the use of streaming data, the \texttt{DATAFLOW} pragma, and doing \texttt{LZW} and \textt{SHA} in parallel), and seeing what additional resources could add to our design.
\newline\par
One thing that could immediately help our design would be to have a fully-associative memory for the records of what the \texttt{SHA} digest was for each chunk index. Putting such a memory into small, local memories would allow for extremely fast lookups in service of deduplication.
\par
By removing the need for shared memory access, that bottleneck could be completely eliminated. Further, it would only take a roughly 5MB memory to do so.
\newline\par
Considering our PL resources, if \texttt{SHA} is the bottleneck, the operation could be distributed across a huge number of computational units. In a similar fashion, given that each chunk is independent from the perspective of the \texttt{LZW} functionality, as many hardware units as possible could be thrown at that problem.
\par
Going further, and expanding on the idea of parallel streams, any issues of transferring the data across FIFO units could be distributed to allow for, for instance, the \texttt{Rabin} computation or \texttt{Deduplication} units to pipeline data through at a quicker speed than would be otherwise possible, writing a larger number of bytes per FPGA "cycle" and further reduce the effective \texttt{II} of our \texttt{DATAFLOW}-pipelined design.


\section{Validation techniques}
As of now the main validation technique is to run a script that can decode the compress.dat file generated from the application and then compares it with the input file that was sent over the network. 

We also use prints on the main arm core, that prints the bytes read by the network thread, to provide real time testing validation that all bytes are being read and no packets are being dropped. 
\section{Key lessons learned}

\section{Design space exploration}

\section{Individual contribution}

\section{Academic code of integrity}
We, Ritika, Taylor and Nishanth, certify that I have complied with the University of Pennsylvania’s Code of Academic Integrity in completing this final exercise.


\end{document}
