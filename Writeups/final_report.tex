\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project Final Report - Group}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle

\section{Introduction}
The elephant in the room for the entirety of our implementation is that, as of 24 hours before the due date of this project, it is not functioning correctly.
It gets through the entirely of the input, and (finally) does not hang in the middle of a deadlock or infinite loop somewhere in hardware, but upon encoding and decoding some files (particuarly, long binary files), the decoded version does not match the original.
\newline\newline
Now, that does not mean we cannot obtain reasonable ideas as to performance, area/time costs, etc. for our design. However, concerns such as specific design axes eluded us through development because we were unable to reach the essential milestone of "a working design."
\newline\newline
This is all to say that, while we cannot empirically verify any results of design changes, or give good accounts of verification strategies, what we \textbf{can} do is evaluate design decisions we could have made through the process, and analyze what kind of effects they would have on a mythical end result where our software worked.
\newline\newline
As such, you must excuse us if we are light on some empiric details, as we, even in this final stretch, endeavor to improve our design to the point of functionality. 90-minute build times are, unfortunately, not condusive to rapid prototyping, so we will do what we can to represent our control of the source material in an academic sense, if not always a practical one.

\section{Single ARM processor design}



\section{Ultra 96 design}
– Performance achieved and energy required
– Compression achieved
– Key design aspects: task decomposition, parallelism, mapping to Zynq resources, include diagrams to support
– Be clear where each component of the final design is performed (e.g., ARM, NEON vector, FPGA logic).
– Model to explain performance, area, and energy of design
– Current bottleneck preventing higher performance

All the components CDC, SHA, LZW and deduplication operate in hardware. 
Input data is read from network and it is processed in 2MB parts. 
// TODO: Detailed description about network reads
\newline\newline

Processor components:
\newline\newline
Input data is read from network and it is processed in 2MB parts. 
\newline
The input read over the network is handled using a pthread that is executing on its own core. 
\newline
The primary processor core(core 0) is running the PS section that interfaces with the PL logic. 
\newline
The network thread is bound to run only on core 2. 
\newline

Network thread functionality: 
\newline\newline
The thread uses a 200MB input buffer that is shared between the cores 2 and 0.
\newline
The NW thread then reads packets, calculates the length of each packet and then copies over the bytes to the buffer.
\newline
In our implementation, we use 2MB buffers to feed the dataflow model implemented on the FPGA.
\newline
So the NW thread has the additional responsibility to increment a counter everytime it crosses the 2MB boundary. 
This variable is checked against in the main processor to make certain that it does not try to process any data before the buffer data fills up. The variable is used as a synchronization mechanism.
\newline
Finally upon exit, the NW thread indicates that it has completed its purpose and marks a variable that is used by core 0 to figure out that no more packets are to be expected.
\newline

Output thread functionality:
\newline\newline
There is a 3rd thread outside of the main and network thread that is tasked with writing processed data onto a file. 
\par
The output thread is bound to core 3 and it is responsible for opening, closing files and of course writing the data to the file. 
It is invoked based on the existing synchronization mechanisms and it frees the processor 0 from having to deal with write backs to the file. 
\newline

CDC performs chunking and the data is taken by SHA and LZW which operate parallely. The output of SHA which is 256 bit SHA value and the compressed output of LZW are both taken by deduplicate section to finally write the appropriate output to the output file. 
3 ARM cores are being used. One is used for reading the input data, one for processing the data and the third for writing the output to the output file.  


\section{10 Gbps design}

As has been noted (TODO: note this somewhere), the fact that we are operating on a \texttt{DATAFLOW} model means that our ability to peek inside the specifics of our design is limited. Xilinx does not like putting that many (read: any) calipers on the internals of our hardware function.\newline
As such, we must examine possibilities for improvements in the abstract. In this section, we will discuss strategies for improving performance which, were we to work as diligently across the next $5$ weeks as we have for this previous $5$, would certainly yield performance improvements in pursuit of a $10Gbps$ goal.
\newline\newline

Looking at our current design, we can approach designing a \texttt{10Gbps} design with the idea that the core functionality would still use logic almost entirely on the FPGA, but we could use more of our resources to solve the problem, and use as many tricks as possible to maximise the utility of the PL's time.
\newline\newline
One of the first things we can see from our current resource usage is that we have a good number of BRAM's still available, as well as a large proportion of our FF/LUT resources.
This opens us up to putting more SHA units (for the computational resources) and LZW units (for the memory resources) onto the PL, with the idea that one of the two (likely the former) is the bottleneck culprit.
\newline
With the idea that a singular \texttt{deduplicate} unit would still need to control output, we could link the \texttt{rabin} unit to the \texttt{deduplicate} unit by way of multiple parallel \texttt{SHA} units; for this example, we will imagine $2$ such units. This would allow us to alternate which unit hashed which series of chunks, allowing for an idealized $2x$ speedup on that axis.
\newline
As for the \texttt{LZW} unit, we could easily fit another onto the PL in the same fashion.
\newline\newline
If the resources are not the bottleneck, then we would need to look into the process by which we are transferring data. the use of \texttt{hls::stream} FIFO units has been handy for automatically handling a lot of timing requirements, but they carry the disadvantage of limiting the rate by which we are (conceptually) pipelining data through our application.
\newline\newline
One way we could improve upon this would be to use multiple streams in parallel for our application; conceptually, this would be similar to, instead of sending water through a single garden hose, sending water through two parallel garden hoses.
\newline\newline
If our bottleneck in the PL is access to the shared memory inside which our SHA dictionary resides, we could mitigate that problem by decreasing the amount of memory we need to read. We could do this by expanding our shared memory hash table region; if we're expecting something along the lines of $2^{16}$ unique chunks in a file, we could allocate $2^{19}$, or more, total hash lines such that we would probabilistically only need to pull one hash line for any given SHA digest.
\newline
In this way, our total memory access would be in the range of $36$ bytes per chunk, rather than our current $144$. The disadvantage would be, we may need a particularly significant field of hash values to near-guarantee that we would not encounter any hash collisions that may break our design.
\newline\newline



\section{Validation techniques}
As of now the main validation technique is to run a script that can decode the compress.dat file generated from the application and then compares it with the input file that was sent over the network. 

We also use prints on the main arm core, that prints the bytes read by the network thread, to provide real time testing validation that all bytes are being read and no packets are being dropped. 
\section{Key lessons learned}

\section{Design space exploration}

Describe design space explored and show graphs and models to support design selection
For our earlier implementation in which we were running only LZW in hardware, the data was being sent to hardware for LZW and then the output back to the processor region to do the next steps in the computation, which led to very low speed of the overall implementation because of this data transfer overhead and most of the part still running in software. 
\newline
But that incremental way of putting things in hardware helped us ultimately moving the entire design to the hardware. So, now all the components CDC, SHA, LZW and deduplication all run in hardware. This removed the delay of transferring intermediate results between the FPGA and the processor region and also increased the overall speed because of hardware acceleration techniques like using pipelining, unrolling, partitioning arrays etc.
\newline\newline
To make advantage of running in hardware. we used dataflow pragma and serialized everything so that as soon as the data is available, the hardware component can start processing on it. 
\newline
CDC provides chunk data to SHA and LZW simultaneously, which operate in parallel. The flow of our design is described by the following diagram:
\newline\newline
// TODO: Add diagram
\newline\newline
To save space on FPGA memory, we use shared memory to store our hash table for the deduplication step.
Another optimization technique was to use separate arm cores to read the input data from ethernet, to process on the data and to write the data to the output file. 
\newline
That saves time in the sense that it doesn't have to wait for the data transmission to complete before it can start processing on the data already read. So, one core reads the data from the ethernet and writes it to the shared memory, another core reads it and transfers it to the FPGA as soon as 2MB data has been written to it, and the third core writes the output data as soon as the 2MB data has been processed in the FPGA. This makes the overall implementation faster. 
\newline\newline

\section{Individual contribution}

\par
Nishanth worked on the Rabin CDC functionality and also implemented the network and output threads on each core. 
\newline\newline
All of us were involved in various capacities in the debug and testing phases of this project.  
\section{Academic code of integrity}
We, Ritika, Taylor and Nishanth, certify that we have complied with the University of Pennsylvania’s Code of Academic Integrity in completing this final exercise.


\end{document}
