\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project Final Report - Group}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle

\section{Introduction}
The elephant in the room for the entirety of our implementation is that, as of 24 hours before the due date of this project, it is not functioning correctly.
It gets through the entirely of the input, and (finally) does not hang in the middle of a deadlock or infinite loop somewhere in hardware, but upon encoding and decoding some files (particuarly, long binary files), the decoded version does not match the original.
\newline\newline
Now, that does not mean we cannot obtain reasonable ideas as to performance, area/time costs, etc. for our design. However, concerns such as specific design axes eluded us through development because we were unable to reach the essential milestone of "a working design."
\newline\newline
This is all to say that, while we cannot empirically verify any results of design changes, or give good accounts of verification strategies, what we \textbf{can} do is evaluate design decisions we could have made through the process, and analyze what kind of effects they would have on a mythical end result where our software worked.
\newline\newline
As such, you must excuse us if we are light on some empiric details, as we, even in this final stretch, endeavor to improve our design to the point of functionality. 90-minute build times are, unfortunately, not condusive to rapid prototyping, so we will do what we can to represent our control of the source material in an academic sense, if not always a practical one.

\section{Single ARM processor design}



\section{Ultra 96 design}
– Performance achieved and energy required
– Compression achieved
– Key design aspects: task decomposition, parallelism, mapping to Zynq resources, include diagrams to support
– Be clear where each component of the final design is performed (e.g., ARM, NEON vector, FPGA logic).
– Model to explain performance, area, and energy of design
– Current bottleneck preventing higher performance

All the components CDC, SHA, LZW and deduplication operate in hardware. 
Input data is read from network and it is processed in 2MB parts. 
// TODO: Detailed description about network reads

CDC performs chunking and the data is taken by SHA and LZW which operate parallely. The output of SHA which is 256 bit SHA value and the compressed output of LZW are both taken by deduplicate section to finally write the appropriate output to the output file. 
3 ARM cores are being used. One is used for reading the input data, one for processing the data and the third for writing the output to the output file.  


\section{10 Gbps design}

\section{Validation techniques}
As of now the main validation technique is to run a script that can decode the compress.dat file generated from the application and then compares it with the input file that was sent over the network. 

We also use prints on the main arm core, that prints the bytes read by the network thread, to provide real time testing validation that all bytes are being read and no packets are being dropped. 
\section{Key lessons learned}

\section{Design space exploration}

\section{Individual contribution}

\section{Academic code of integrity}
We, Ritika, Taylor and Nishanth, certify that I have complied with the University of Pennsylvania’s Code of Academic Integrity in completing this final exercise.


\end{document}
