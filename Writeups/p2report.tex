\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project P2 Report}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle


\section{Design Space Axes}
\begin{enumerate}
\item%1
\textbf{CDC:}
\begin{enumerate}
\item%a
\textbf{Axis:} Multiple CDC HW resources
\newline
\textbf{Challenge:} Improving throughput of chunks created from input file. Ideally linear increase in throughput. 
\newline
\textbf{Opportunity:} Not a viable opportunity, as although the input space can be divided up for each resource, the chunks generated from the split stages cannot be utilized by later stages until all chunks in the first resource have been computed.
\newline
\textbf{Continuum:} From 1 to how many can fit into fabric. At higher resource numbers, the chunkable data stream would reduce thus creating more overhead in LZW compression. This probably creates diminishing returns.
\newline
\textbf{Equation for Benefit:} Total throughput = (No.of resources * Throughput of individual CDC)
\item%b
\textbf{Axis:} Using vector engines for computation
\newline
\textbf{Challenge: } Improving throughput by leveraging independent data level parallelism
\newline
\textbf{Opportunity: } CDC design does not give this opportunity because data is operated in a byte wise manner. Secondly, there is no data independent operation as start of next chunk depends on end of previous chunk. Hard to define independent boundaries.
\newline
\textbf{Continuum:} Dividing data as 8x16bit blocks or 4x32 etc. 
\newline
\textbf{Equation for benefit:} A linear improvement as earlier:
\newline
N = no.of vector lanes, then N * computation on a single lane
\item%c
\textbf{Axis:} Using pipelining stages on microlevel
\newline
\textbf{Challenge:} Allowing CPI of 1, removing sequential constraints.
\newline
\textbf{Opportunity:} CDC design isn't ideal for pipelining. Although it works at a byte level granularity with sequential ordered access, where each byte is slid into the window and hash is recalculated for the new window, there is a cyclic dependency with the hash digests between stages due to the rolling hash implementation. This prevents an achievable II of 1. 
\newline
\textbf{Continuum:} Even from 2 stage to N stages, the algorithm does not allow for full pipeline utilization.
\newline
\textbf{Equation for benefit:} Throughput increases based on depth of pipeline, in this case, for N stages, Nx increase in calculating chunk boundary throughput.
\item%d
\textbf{Axis:} Using pipelining stages on macro level
\newline
\textbf{Challenge:} Implementing a dataflow within HW functions to prevent memory accesses and efficient flow of data so that all the HW functions in effect are computing every cycle and thus pipelined.
\newline
\textbf{Opportunity:} Identifying size of local stores to be used as a RAM interface or using streaming the bytes to next level but figuring out a mechanism to indicate chunk boundary. 
\newline
\textbf{Continuum:} Using a large RAM interface creates a memory bottleneck, if partitioning the interface, it uses up FPGA resources.
Streaming removes this continuum equation as its all at a byte granularity.
\newline
\textbf{Equation for benefit: } Throughput improves at a macro level due to reduction in memory access time and allowing all functions to be busy with computations effectively utilising the parallel nature of the FPGA.
\item%e
\textbf{Axis:} Data mover selection and data access pattern
\newline
\textbf{Challenge:} Selecting the correct data mover between the PS and PL sections and in the process ensuring hardware code is altered to suit the data mover and conforms to the data access pattern. 
\newline
\textbf{Opportunity:} Using the right data mover for the hardware function arguments is important in order to reduce resource usage and improve efficiency. Example for small scalar arguments, an AXI LITE data mover would be the right choice.
For array and structure arguments containing arrays, the choices can be multiple, it can use AXIDMA SIMPLE or AXIDMA SG movers for normal data copy. 
If we are using shared memory, then an AXI master interface is used. Shared memory access should use buffering of data so that data can be transferred in bursts rather than a single byte access to DDR memory.
The data access pattern ensures that for an ap fifo(sequential) interface, data is accessed in order and only once. For random access pattern, a normal data copy can be used. 
A sequential interface is ideal for byte accesses in order, e.g. for CDC and LZW. However the code needs to be restructured, such as using local stores like registers and line buffers to enable streaming. 
\newline
\textbf{Continuum:} Depending on the data length and number of accesses of the data needed by the HW function, the data mover can vary from the lightweight AXI LITE to a more complicated AXIDMA SG. These factors need to be considered and interfacing code restructured to support the appropriate data mover.
\newline
\item%f
\textbf{Axis:} Optimizing memory accesses. 
\newline
\textbf{Challenge: } As touched upon above, this deals with efficient usage of local store to alleviate DRAM accesses which bottleneck FPGA. Using clever local stores and initialization to prevent DRAM penalty of million cycles.
\newline
\textbf{Opportunity: } A great mechanism to prevent computation as well as memory redundancies. For example, if the same computation is repeated multiple times, its efficient to store the output locally and reuse it. 
For memory accesses, reusing memory already read from DRAM by storing it locally prevents data memory re-access and heavy data movement penalties. 
Local store can also be an stop gap between hardware functions in a dataflow setup, once more alleviating DRAM access.
\newline
\textbf{Continuum: } Depending on how much data needs to be stored locally, registers with multi port accesses can be an approach, or if a large BRAM is required then, memory ports may turn into bottlenecks. These factors should be considered. 
\newline
\textbf{Equation for benefit: } saves about million clock cycles per redundant memory access.
\item%g
\textbf{Axis:} Resource Usage
\newline
\textbf{Challenge: }  Limiting resource usage to fit within the FPGA provided resources.
\newline
\textbf{Opportunity: } BRAMs provide a convenient local store for large data types, but also introduce memory bottlenecks in the form of limited read/write ports. 
Using array partitions, data can be moved across BRAMs or even stored in registers but with its own downsides.
\newline
\textbf{Continuum: } Large data stores use BRAMs, but memory bottleneck.
Smaller data stores remove bottlenecks but cause heavy resource usage and long FPGA synthesis times.
\newline
\textbf{Equations for benefit:} memory port bottleneck = constant * memory store size
\newline
 \item%h
\textbf{Axis: } Mapping tasks to CPU, FPGA
\newline
\textbf{Challenge: } The individual functions can be split among CPU cores and FPGA logic.
\newline
\textbf{Opportunity: } Deciding if data level parallelism can be exploited on the CPU Vector engine, perhaps for SHA 512 bit blocks. 
Maybe having multi core usage of a functionality so that each core forms a stage in a micro pipeline and the associated syncing issues present with such an approach.
CPUs also can benefit from cacheability and higher clock speeds for computations. 
\newline
\textbf{Continuum: } How many stages split across multiple cores and the complexity of synchronization between cores as the number of stages increases.
\newline
\textbf{Equations for benefit:} Ideally, for N stages, a linear increase in throughput and at higher clock speeds, thus improving on baseline CPU performance.  
\newline
\newline
\end{enumerate}

\item%2
\textbf{SHA:}
\begin{enumerate}
\item%a
\textbf{Axis:} $S$, Number of SHA-256 hardware units
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Send chunks to rotating SHA unit index to allow for parallel execution
\newline
\textbf{Continuum:} Anywhere from $1$ to however many of our hardware SHA units will fit on the FPGA
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=S*\texttt{singleSHAUnitThroughput}$

\item%b

\textbf{Axis:} $K$, No. of SHA sub-chunk computation unit for computing on 64 byte sub-chunks
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Send sub chunks of 64 bytes to sub-chunk computation units for parallel execution and each stores its result which can all be added to get the final hash values
\newline
\textbf{Continuum:} Anywhere from $1$ to chunk size divided by 64 bytes(size of 1 sub-chunk).  
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=K*\texttt{singleSHASubUnitThroughput}$

\item%c

\textbf{Axis:} $P$, Type of memory(units of partitioned memory) to store the input chunk data of SHA
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Partition the input chunk array so that they can be read simultaneously to send over to sub-chunk computation unit
\newline
\textbf{Continuum:} Anywhere from a depth of $1$ to $64$ which is the sub-chunk size so that each sub-chunk input data can be read from memory simultaneously. Makes more sense to have a depth of 128 because there are 2 ports and that's how input data for computation of 2 sub chunks can be read simulatneously.    
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=P*\texttt{singleChunkMemReadTime}$

\item%d

\textbf{Axis:} $H$, Type of memory to store the hash values
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Partition the array storing eight 32-bit values so that they can be read/written simultaneously
\newline
\textbf{Continuum:} Anywhere from $1$ to $8$.  
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=M*\texttt{singleMemRead/Write}$

\item%e

\textbf{Axis:} $A$, Pipeline depth for hashing   
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Pipeline hashing function so that computation for next chunk can start when current is still being computed
\newline
\textbf{Continuum:} Anything from 2 to a value before or at which pipelining benefits can be observed. After that value, it will start to increase the latency due to pipleine overheads as it results in diminishing returns.
\newline
\textbf{Equation for Benefit:}  It increases the throughput. If earlier it was taking n cycles for an output, then with careful setting of pipeline depth and pipeline stages, it has the ability to result in 1 output per cycle.

\item%f

\textbf{Axis:} $DM$, Data movement between sub-chunk computation units of SHA - save on memory
\newline
\textbf{Challenge:} Improving latency of SHA computation for a chunk and saving memory to store intermediate hash values
\newline
\textbf{Opportunity:} Stream intermediate hash values from 1 unit to another in order to do perform addition to generate the final hash values 
\newline
\textbf{Continuum:} The continuum here is basically about how much memory to be used as FIFO which can vary from 1 32-bit location to 8 of them. The latter is simply the case where all the 8 hash values are being stored on memory, nothing really being streamed. 
\newline
\textbf{Equation for Benefit:}  For n 64 byte subchunks, memory utilization would be 32*n bytes. With streaming, it could go as low as 1 byte to no memory at all. 

\end{enumerate}
\item%3
\textbf{LZW:}
\begin{enumerate}
\item%a

\textbf{Axis:} $L$, Number of LZW hardware units
\newline
\textbf{Challenge:} Improving throughput of LZW step
\newline
\textbf{Opportunity:} Send chunks to rotating LZE unit index to allow for parallel execution
\newline
\textbf{Continuum:} Anywhere from $1$ to however many of our hardware LZW units will fit on the FPGA (BRAM likely limiting factor)
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(L\right)=S*\texttt{singleLZWUnitThroughput}$

\item%b

\textbf{Axis:} $Z$, Design choice for LZW hash table unit
\newline
\textbf{Challenge:} Allow for efficient access of code-table for LZW step while fitting within hardware specifications
\newline
\textbf{Opportunity:} Use trees or associative memories (or both) to allow for low cycle count for finding relevant table entry
\newline
\textbf{Continuum:} $Z\in\{$Tree with Dense RAM, Tree with Fully Associative Memory, Tree with Tree, Tree with Hybrid$\}$
\newline
\textbf{Equation for Benefit:} Slide 65 from Day 17 has the relevant tradeoff chart, with implied \texttt{implementation\_complexity} parameter to consider.

\item%c

\textbf{Axis:} $II_L$, Pipelining II for LZW hardware implementation
\newline
\textbf{Challenge:} Allow for quick compression algorithm
\newline
\textbf{Opportunity:} Loosen pipelining constraints for LZW to reduce computational load
\newline
\textbf{Continuum:} $1$ to \texttt{MAX\_CHUNK\_SIZE}
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(\texttt{LZW}\right)=\frac{1 byte}{II_L}$

\item%d

\textbf{Axis:} $W_L$, LZW compression window size
\newline
\textbf{Challenge:} Cut down on LZW memory requirements
\newline
\textbf{Opportunity:} Restructure how encoding/decoding interprets data to reduce conceptual table depth from \texttt{MAX\_CHUNK\_SIZE} rows down to some smaller $W_L$
\newline
\textbf{Continuum:} \texttt{MAX\_CHUNK\_SIZE} to $1$ (the latter of which would make it stop being compression)
\newline
\textbf{Equation for Benefit:} $\texttt{memRequirements}_{LZW} *= \frac{W_L}{\texttt{MAX\_CHUNK\_SIZE}}$\newline
Note: there are a number of things this change would affect, which is also highly dependent on $Z$ (defined above). We will likely not change this, but it is a parameter that could be tuned.

\item%e

\textbf{Axis:} $D_L$, Pipeline depth into LZW implementation
\newline
\textbf{Challenge:} Reduce chances of idle LZW unit
\newline
\textbf{Opportunity:} Lengthen pipeline depth so that variable chunk size does not prevent parallel execution and effective pipelining between CDC and LZW
\newline
\textbf{Continuum:} $1$ byte to $1$MB
\newline
\textbf{Equation for Benefit:} Likely complex and related to a lot of interlocking features (no equation provided)

\item%f

\textbf{Axis:} $HM_{L}$, Hash Method for LZW string storage
\newline
\textbf{Challenge:} Efficient and effective storage of code strings in LZW calculation
\newline
\textbf{Opportunity:} Find a collective hash function that allows for byte-wise calculation of string hash as string grows
\newline
\textbf{Continuum:} XOR'ing bits, modular addition, any of a variety of other hash methods
\newline
\textbf{Equation for Benefit:} Varies by method; in general, will need to evaluate computational complexity for hash-uniformity against penalties for hash collisions. Analysis would likely be better served in an empirical, rather than theoretical, space.


\end{enumerate}
\item%4
\textbf{Interfacing:}

\begin{enumerate}
\item%a

\textbf{Axis:} $N$, Number of bytes at a time transfered to CDC unit
\newline
\textbf{Challenge:} Balance memory transfer overhead against memory storage for incoming data
\newline
\textbf{Opportunity:} Loosen pipelining constraints for LZW to reduce computational loa
\newline
\textbf{Continuum:} $1$ to $1MB$ (this could be a fake limit)
\newline
\textbf{Equation for Benefit:} $\texttt{memTransferTime}_{total} = \frac{\texttt{totalInput}}{N}*\left(\texttt{memTransferOverhead} + N*\texttt{memTransferRate}    \right)$

\item%b

\textbf{Axis:} $O$ Memory requirement for sending data from CDC to SHA and LZW 
\newline
\textbf{Challenge:} Reduce the amount of memory required to save chunk data after CDC to be input into SHA and LZW which could be 8KB in the worst case.
\newline
\textbf{Opportunity:} Stream the chunk data from CDC into LZW and SHA
\newline
\textbf{Continuum:} memory requirement for streaming data could vary from using just 1 byte to a few bytes of FIFO.  
\newline
\textbf{Equation for Benefit:} Saves memory from using $MAX_CHUNK(8KB)$ amount of memory to just a few bytes

\item%c

\textbf{Axis:} $P$, Granularity of data that can be sent from one unit to the next in the application flow
\newline
\textbf{Challenge:} Reduce the time spent on waiting for input until the previous stage is done computing its output  
\newline
\textbf{Opportunity:} Use the DATAFLOW pragma to make use of the input data as soon as it is available from the previous stage 
\newline
\textbf{Continuum:} Could stream one stage's output to next stage's input with a granularity of chunk size to sending data as soon as $1$ byte is available 
\newline
\textbf{Equation for Benefit:} saves on time spent in waiting for the input data to be available as now it can start operating as soon as $1$ byte of input data is available

\item%d

\textbf{Axis:} $H$, Number of bits in hash of SHA value for storing SHA values
\newline
\textbf{Challenge:} Effectively storing mapping between SHA values of previous chunks and the chunk index
\newline
\textbf{Opportunity:} Tune hash table size to reduce conflicts but also remain compact
\newline
\textbf{Continuum:} Could be any small number of bits (call it $5$ as a low value) through $256$ for the full SHA value.
\newline
\textbf{Equation for Benefit:}
\[
\texttt{numRows }C=2^H
\]
\[
\texttt{probCollision}={N \choose m}\left(\frac{1}{C}\right)^m \left(1 - \frac{1}{C}\right)^{N-m}
\]


\section{Placeholder Refinement}
All the components are fully functional and produce the correct output when run on arm core. 

\section{Zip file submission}
Zip file of fully functional code submitted.

\section{Binaries}
Zip file of binaries submitted.

\section{Documentatiion}



\begin{enumerate}
\item%a 
All the source codes have been documented and the references have been documented. 



\item%b
\textbf{Compression ratio:}
\newline
\textbf{Breakdown:}



\item%c

\textbf{Overall thriughput:}

\item%d

\textbf{Validation:}

\item%e

\textbf{Contribution:} Nishanth has taken the lead on the CDC section, Ritika has led the progress for SHA, and Taylor has taken on the LZW portion of the project so far. The overall design and interfacing came from a collective meeting towards the top of the process, with incremental changes happening in subsequent meetings.

\end{enumerate}

\section{Challenges in collboration and integration}

With at least one team member working remotely, collaboration across distance can be tricky. We have a slack channel that helps with overall communication, but there's no great substitute for on-site collaboration.
\newline
Perhaps the biggest obstacle to integration and implementation this week was the workload of developing 20-30 design axes, as opposed to a world where we could really start putting our actual hardware implementation together. That said, we ended last week with our single-core CPU code close to working, so the implementation changes we've needed to enact were, fortunately, not too heavy of a load.

\end{enumerate}

\end{enumerate}

\begin{appendices}
%\section{2m Filter.cpp}\label{2m}
%\lstinputlisting[language=C]{code/Filter.cpp}
%\section{1h mmult\_accel.cpp}\label{1hB}
%\lstinputlisting[language=C]{code/mmult_accel.cpp}
%\section{1h mmult\_accel.h}\label{1hC}
%\lstinputlisting[language=C]{code/mmult_accel.h}


\end{appendices}





\end{document}
