\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project P2 Report}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle


\section{Design Space Axes}
\begin{enumerate}
\item%1

\textbf{Axis:} $S$, Number of SHA-256 hardware units
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Send chunks to rotating SHA unit index to allow for parallel execution
\newline
\textbf{Continuum:} Anywhere from $1$ to however many of our hardware SHA units will fit on the FPGA
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=S*\texttt{singleSHAUnitThroughput}$

\item%2

\textbf{Axis:} $L$, Number of LZW hardware units
\newline
\textbf{Challenge:} Improving throughput of LZW step
\newline
\textbf{Opportunity:} Send chunks to rotating LZE unit index to allow for parallel execution
\newline
\textbf{Continuum:} Anywhere from $1$ to however many of our hardware LZW units will fit on the FPGA (BRAM likely limiting factor)
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(L\right)=S*\texttt{singleLZWUnitThroughput}$

\item%3

\textbf{Axis:} $Z$, Design choice for LZW hash table unit
\newline
\textbf{Challenge:} Allow for efficient access of code-table for LZW step while fitting within hardware specifications
\newline
\textbf{Opportunity:} Use trees or associative memories (or both) to allow for low cycle count for finding relevant table entry
\newline
\textbf{Continuum:} $Z\in\{$Tree with Dense RAM, Tree with Fully Associative Memory, Tree with Tree, Tree with Hybrid$\}$
\newline
\textbf{Equation for Benefit:} Slide 65 from Day 17 has the relevant tradeoff chart, with implied \texttt{implementation\_complexity} parameter to consider.

\item%4

\textbf{Axis:} $H$, Number of bits in hash of SHA value for storing SHA values
\newline
\textbf{Challenge:} Effectively storing mapping between SHA values of previous chunks and the chunk index
\newline
\textbf{Opportunity:} Tune hash table size to reduce conflicts but also remain compact
\newline
\textbf{Continuum:} Could be any small number of bits (call it $5$ as a low value) through $256$ for the full SHA value.
\newline
\textbf{Equation for Benefit:}
\[
\texttt{numRows }C=2^H
\]
\[
\texttt{probCollision}={N \choose m}\left(\frac{1}{C}\right)^m \left(1 - \frac{1}{C}\right)^{N-m}
\]

\item%5

\textbf{Axis:} $K$, No. of sub-chunk computation unit for computing on 64 byte sub-chunks
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Send sub chunks of 64 bytes to sub-chunk computation units for parallel execution and each stores its result which can all be added to get the final hash values
\newline
\textbf{Continuum:} Anywhere from $1$ to chunk size divided by 64 bytes(size of 1 sub-chunk).  
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=K*\texttt{singleSHASubUnitThroughput}$

\item%6

\textbf{Axis:} $P$, Type of memory(units of partitioned memory) to store the input chunk data
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Partition the input chunk array so that they can be read simultaneously to send over to sub-chunk computation unit
\newline
\textbf{Continuum:} Anywhere from a depth of $1$ to $64$ which is the sub-chunk size so that each sub-chunk input data can be read from memory simultaneously. Makes more sense to have a depth of 128 because there are 2 ports and that's how input data for computation of 2 sub chunks can be read simulatneously.    
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=P*\texttt{singleChunkMemReadTime}$

\item%7

\textbf{Axis:} $M$, Type of memory to store the hash values  
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Partition the array storing eight 32-bit values so that they can be read/written simultaneously
\newline
\textbf{Continuum:} Anywhere from $1$ to $8$.  
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=M*\texttt{singleMemRead/Write}$

\item%8

\textbf{Axis:} $II_L$, Pipelining II for LZW hardware implementation
\newline
\textbf{Challenge:} Allow for quick compression algorithm
\newline
\textbf{Opportunity:} Loosen pipelining constraints for LZW to reduce computational load
\newline
\textbf{Continuum:} $1$ to \texttt{MAX\_CHUNK\_SIZE}
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(\texttt{LZW}\right)=\frac{1 byte}{II_L}$

\item%9

\textbf{Axis:} $W_L$, LZW compression window size
\newline
\textbf{Challenge:} Cut down on LZW memory requirements
\newline
\textbf{Opportunity:} Restructure how encoding/decoding interprets data to reduce conceptual table depth from \texttt{MAX\_CHUNK\_SIZE} rows down to some smaller $W_L$
\newline
\textbf{Continuum:} \texttt{MAX\_CHUNK\_SIZE} to $1$ (the latter of which would make it stop being compression)
\newline
\textbf{Equation for Benefit:} $\texttt{memRequirements}_{LZW} *= \frac{W_L}{\texttt{MAX\_CHUNK\_SIZE}}$\newline
Note: there are a number of things this change would affect, which is also highly dependent on $Z$ (defined above). We will likely not change this, but it is a parameter that could be tuned.

\item%10

\textbf{Axis:} $N$, Number of bytes at a time transfered to CDC unit
\newline
\textbf{Challenge:} Balance memory transfer overhead against memory storage for incoming data
\newline
\textbf{Opportunity:} Loosen pipelining constraints for LZW to reduce computational loa
\newline
\textbf{Continuum:} $1$ to $1MB$ (this could be a fake limit)
\newline
\textbf{Equation for Benefit:} $\texttt{memTransferTime}_{total} = \frac{\texttt{totalInput}}{N}*\left(\texttt{memTransferOverhead} + N*\texttt{memTransferRate}    \right)$

\item%11

\textbf{Axis:} $D_L$, Pipeline depth into LZW implementation
\newline
\textbf{Challenge:} Reduce chances of idle LZW unit
\newline
\textbf{Opportunity:} Lengthen pipeline depth so that variable chunk size does not prevent parallel execution and effective pipelining between CDC and LZW
\newline
\textbf{Continuum:} $1$ byte to $1$MB
\newline
\textbf{Equation for Benefit:} Likely complex and related to a lot of interlocking features (no equation provided)

\item%12

\end{enumerate}


\section{Teamwork}


\begin{appendices}
%\section{2m Filter.cpp}\label{2m}
%\lstinputlisting[language=C]{code/Filter.cpp}
%\section{1h mmult\_accel.cpp}\label{1hB}
%\lstinputlisting[language=C]{code/mmult_accel.cpp}
%\section{1h mmult\_accel.h}\label{1hC}
%\lstinputlisting[language=C]{code/mmult_accel.h}


\end{appendices}





\end{document}
