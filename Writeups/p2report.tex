\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project P2 Report}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle


\section{Design Space Axes}
\begin{enumerate}
\item%1

\textbf{CDC:}
\item%1
\textbf{Axis:} Multiple CDC HW resources
\newline
\textbf{Challenge:} Improving throughput of chunks created from input file. Ideally linear increase in throughput. 
\newline
\textbf{Opportunity:} Not a viable opportunity, as although the input space can be divided up for each resource, the chunks generated from the split stages cannot be utilized by later stages until all chunks in the first resource have been computed.
\newline
\textbf{Continuum:} From 1 to how many can fit into fabric. At higher resource numbers, the chunkable data stream would reduce thus creating more overhead in LZW compression. This probably creates diminishing returns.
\newline
\textbf{Equation for Benefit:} Total throughput = (No.of resources * Throughput of individual CDC)
\item%2
\textbf{Axis:} Using vector engines for computation
\newline
\textbf{Challenge: } Improving throughput by leveraging independent data level parallelism
\newline
\textbf{Opportunity: } CDC design does not give this opportunity because data is operated in a byte wise manner. Secondly, there is no data independent operation as start of next chunk depends on end of previous chunk. Hard to define independent boundaries.
\newline
\textbf{Continuum:} Dividing data as 8x16bit blocks or 4x32 etc. 
\newline
\textbf{Equation for benefit:} A linear improvement as earlier:
\newline
N = no.of vector lanes, then N * computation on a single lane
\item%3
\textbf{Axis:} Using pipelining stages on microlevel
\newline
\textbf{Challenge:} Allowing CPI of 1, removing sequential constraints.
\newline
\textbf{Opportunity:} CDC design is great for pipelining since it works at a byte level granularity. Each byte is slid into the window and hash is recalculated for the new window. Finally the rabin digest is compared for a pattern to identify chunk boundary.
Thus the 4 stages are:
Read input byte from memory or as a stream.
Slide into window.
Calculate window digest
Check for boundary condition.
\newline
\textbf{Continuum:} 2 stage to N stages. 2 stages do not allow for full pipeline utilization. Infinite stages cannot be supported because there is a high granularity which cause data dependency and stalls. Can probably be mitigated with local stores, but need to explore.
\newline
\textbf{Equation for benefit:} Throughput increases based on depth of pipeline, in this case 4x increase in calculating chunk boundary.
\item%4
\textbf{Axis:} Using pipelining stages on macro level
\newline
\textbf{Challenge:} Implementing a dataflow within HW functions to prevent memory accesses and efficient flow of data so that all the HW functions in effect are computing every cycle and thus pipelined.
\newline
\textbf{Opportunity:} Identifying size of local stores to be used as a RAM interface or using streaming the bytes to next level but figuring out a mechanism to indicate chunk boundary. 
\newline
\textbf{Continuum:} Using a large RAM interface creates a memory bottleneck, if partitioning the interface, it uses up FPGA resources.
Streaming removes this continuum equation as its all at a byte granularity.
\newline
\textbf{Equation for benefit: } Throughput improves at a macro level due to reduction in memory access time and allowing all functions to be busy with computations effectively utilising the parallel nature of the FPGA.
\item%5

\item%6


\textbf{SHA:}
\item%1

\textbf{Axis:} $S$, Number of SHA-256 hardware units
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Send chunks to rotating SHA unit index to allow for parallel execution
\newline
\textbf{Continuum:} Anywhere from $1$ to however many of our hardware SHA units will fit on the FPGA
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=S*\texttt{singleSHAUnitThroughput}$

\item%2

\textbf{Axis:} $K$, No. of SHA sub-chunk computation unit for computing on 64 byte sub-chunks
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Send sub chunks of 64 bytes to sub-chunk computation units for parallel execution and each stores its result which can all be added to get the final hash values
\newline
\textbf{Continuum:} Anywhere from $1$ to chunk size divided by 64 bytes(size of 1 sub-chunk).  
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=K*\texttt{singleSHASubUnitThroughput}$

\item%3

\textbf{Axis:} $P$, Type of memory(units of partitioned memory) to store the input chunk data of SHA
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Partition the input chunk array so that they can be read simultaneously to send over to sub-chunk computation unit
\newline
\textbf{Continuum:} Anywhere from a depth of $1$ to $64$ which is the sub-chunk size so that each sub-chunk input data can be read from memory simultaneously. Makes more sense to have a depth of 128 because there are 2 ports and that's how input data for computation of 2 sub chunks can be read simulatneously.    
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=P*\texttt{singleChunkMemReadTime}$

\item%4

\textbf{Axis:} $M$, Type of memory to store the hash values  
\newline
\textbf{Challenge:} Improving throughput of hashing step
\newline
\textbf{Opportunity:} Partition the array storing eight 32-bit values so that they can be read/written simultaneously
\newline
\textbf{Continuum:} Anywhere from $1$ to $8$.  
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(S\right)=M*\texttt{singleMemRead/Write}$

\item%5


\item%6


\textbf{LZW:}
\item%1

\textbf{Axis:} $L$, Number of LZW hardware units
\newline
\textbf{Challenge:} Improving throughput of LZW step
\newline
\textbf{Opportunity:} Send chunks to rotating LZE unit index to allow for parallel execution
\newline
\textbf{Continuum:} Anywhere from $1$ to however many of our hardware LZW units will fit on the FPGA (BRAM likely limiting factor)
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(L\right)=S*\texttt{singleLZWUnitThroughput}$

\item%2

\textbf{Axis:} $Z$, Design choice for LZW hash table unit
\newline
\textbf{Challenge:} Allow for efficient access of code-table for LZW step while fitting within hardware specifications
\newline
\textbf{Opportunity:} Use trees or associative memories (or both) to allow for low cycle count for finding relevant table entry
\newline
\textbf{Continuum:} $Z\in\{$Tree with Dense RAM, Tree with Fully Associative Memory, Tree with Tree, Tree with Hybrid$\}$
\newline
\textbf{Equation for Benefit:} Slide 65 from Day 17 has the relevant tradeoff chart, with implied \texttt{implementation\_complexity} parameter to consider.

\item%3

\textbf{Axis:} $II_L$, Pipelining II for LZW hardware implementation
\newline
\textbf{Challenge:} Allow for quick compression algorithm
\newline
\textbf{Opportunity:} Loosen pipelining constraints for LZW to reduce computational load
\newline
\textbf{Continuum:} $1$ to \texttt{MAX\_CHUNK\_SIZE}
\newline
\textbf{Equation for Benefit:} \texttt{Throughput}$\left(\texttt{LZW}\right)=\frac{1 byte}{II_L}$

\item%4

\textbf{Axis:} $W_L$, LZW compression window size
\newline
\textbf{Challenge:} Cut down on LZW memory requirements
\newline
\textbf{Opportunity:} Restructure how encoding/decoding interprets data to reduce conceptual table depth from \texttt{MAX\_CHUNK\_SIZE} rows down to some smaller $W_L$
\newline
\textbf{Continuum:} \texttt{MAX\_CHUNK\_SIZE} to $1$ (the latter of which would make it stop being compression)
\newline
\textbf{Equation for Benefit:} $\texttt{memRequirements}_{LZW} *= \frac{W_L}{\texttt{MAX\_CHUNK\_SIZE}}$\newline
Note: there are a number of things this change would affect, which is also highly dependent on $Z$ (defined above). We will likely not change this, but it is a parameter that could be tuned.

\item%5

\textbf{Axis:} $D_L$, Pipeline depth into LZW implementation
\newline
\textbf{Challenge:} Reduce chances of idle LZW unit
\newline
\textbf{Opportunity:} Lengthen pipeline depth so that variable chunk size does not prevent parallel execution and effective pipelining between CDC and LZW
\newline
\textbf{Continuum:} $1$ byte to $1$MB
\newline
\textbf{Equation for Benefit:} Likely complex and related to a lot of interlocking features (no equation provided)

\item%6


\textbf{Interfacing:}
\item%1

\textbf{Axis:} $N$, Number of bytes at a time transfered to CDC unit
\newline
\textbf{Challenge:} Balance memory transfer overhead against memory storage for incoming data
\newline
\textbf{Opportunity:} Loosen pipelining constraints for LZW to reduce computational loa
\newline
\textbf{Continuum:} $1$ to $1MB$ (this could be a fake limit)
\newline
\textbf{Equation for Benefit:} $\texttt{memTransferTime}_{total} = \frac{\texttt{totalInput}}{N}*\left(\texttt{memTransferOverhead} + N*\texttt{memTransferRate}    \right)$

\item%2

\textbf{Axis:} $O$ Memory requirement for sending data from CDC to SHA and LZW 
\newline
\textbf{Challenge:} Reduce the amount of memory required to save chunk data after CDC to be input into SHA and LZW which could be 8KB in the worst case.
\newline
\textbf{Opportunity:} Stream the chunk data from CDC into LZW and SHA
\newline
\textbf{Continuum:} memory requirement for streaming data could vary from using just 1 byte to a few bytes of FIFO.  
\newline
\textbf{Equation for Benefit:} Saves memory from using $MAX_CHUNK(8KB)$ amount of memory to just a few bytes

\item%3

\textbf{Axis:} $P$, Granularity of data that can be sent from one unit to the next in the application flow
\newline
\textbf{Challenge:} Reduce the time spent on waiting for input until the previous stage is done computing its output  
\newline
\textbf{Opportunity:} Use the DATAFLOW pragma to make use of the input data as soon as it is available from the previous stage 
\newline
\textbf{Continuum:} Could stream one stage's output to next stage's input with a granularity of chunk size to sending data as soon as $1$ byte is available 
\newline
\textbf{Equation for Benefit:} saves on time spent in waiting for the input data to be available as now it can start operating as soon as $1$ byte of input data is available

\item%4

\textbf{Axis:} $H$, Number of bits in hash of SHA value for storing SHA values
\newline
\textbf{Challenge:} Effectively storing mapping between SHA values of previous chunks and the chunk index
\newline
\textbf{Opportunity:} Tune hash table size to reduce conflicts but also remain compact
\newline
\textbf{Continuum:} Could be any small number of bits (call it $5$ as a low value) through $256$ for the full SHA value.
\newline
\textbf{Equation for Benefit:}
\[
\texttt{numRows }C=2^H
\]
\[
\texttt{probCollision}={N \choose m}\left(\frac{1}{C}\right)^m \left(1 - \frac{1}{C}\right)^{N-m}
\]

\end{enumerate}


\section{Teamwork}


\begin{appendices}
%\section{2m Filter.cpp}\label{2m}
%\lstinputlisting[language=C]{code/Filter.cpp}
%\section{1h mmult\_accel.cpp}\label{1hB}
%\lstinputlisting[language=C]{code/mmult_accel.cpp}
%\section{1h mmult\_accel.h}\label{1hC}
%\lstinputlisting[language=C]{code/mmult_accel.h}


\end{appendices}





\end{document}
