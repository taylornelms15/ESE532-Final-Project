\documentclass{article}
\usepackage[margin=0.5in]{geometry}

\usepackage{listings}
\usepackage{enumitem}
\usepackage{appendix}
\usepackage{graphicx}


\title{ESE532 Project P4 Report - Group}
\author{Ritika Gupta, Taylor Nelms, and Nishanth Shyamkumar}

\begin{document}

\maketitle


\section{Deduplication and compression}
\begin{enumerate}
\item%a
\textbf{Throughput}: The throughput is 10Mbps. The throughput decreased from 15 to 10 by putting rabin, SHA also in hardware because 
Rabin: The input and output to the rabin function in the current implementation is not ideal because 
a)It does not know the length of chunk beforehand, within a stream of 8K bytes. This means that the Rabin always has to run a full 8K cycles for every single chunk even if the chunk boundary is detected at 2K etc.
b) Since the data input was malloc'ed it needed to use DMA SG as the data mover, which incurs a high penalty on each run. Also the idea of running Rabin in HW for each chunk and then moving it out back to PS for sending to the next stage causes a high data movement penalty across the system. 
\newline\newline
We are also moving towards a \texttt{dataflow} model to stream data between our components. However, this has involved a lot of late-game integration issues, and we are still troubleshooting these concerns. Out current \texttt{dataflow} model puts in placeholder functions for Rabin and SHA units (outputting random information for the SHA digest and using fixed chunk sizes for the Rabin chunking), and is able to achieve a higher throughput than the current implementation. We hope that this will allow for a better resultant throughput, as the movement of data will not be dependent upon CPU data movers, and will hopefully pipeline our data through the different modules sensibly.
\newline

\item%b
\textbf{Compression status}: Current compression status for vmlinuz.tar is 44 percent for an original file size of 66MB and it compressed it to 37MB. For little prince, the compression ratio is 67 percent with the original file size of 14KB and the compressed file size 4.5KB. For a file not suitable for deduplication like Franklin, the compression ratio is 21 percent for an original file size of 390K and the compressed file size 308K.
\newline

\item%c
\textbf{Validations}: We use decoder to produce the uncompressed file and then compare it with the original uncompressed file and they match. 
We also compare the software run output(Golden output) with the HW run output to check for validity. 
\newline

\item%d
\textbf{Design space}\newline
Rabin CDC:\newline

SHA:\newline

LZW:\newline


\newline

\item%e
\textbf{Techniques for speedup}: We put all three components in hardware this time to increase the speed. It saves time to send data from PS to PL and then back. 
\newline


\item%f
\textbf{Perfomance model}
Rabin CDC:


SHA:


LZW:
\newline


\item%b
\textbf{Task distribution}: 
Ritika did the SHA logic on HW implementation. 
Nishanth did the Rabin logic on HW implementation.
Taylor has done the LZW Compression logic on HW implementation, along with the groundwork for a better-streaming interfacing across future implementations.
Chunkdict is currently running on Software. 
\newline

\section{Code}

Included in different turn-in location.

\section{Binaries}

Included in different turn-in location.


\begin{appendices}
%\section{2m Filter.cpp}\label{2m}
%\lstinputlisting[language=C]{code/Filter.cpp}
%\section{1h mmult\_accel.cpp}\label{1hB}
%\lstinputlisting[language=C]{code/mmult_accel.cpp}
%\section{1h mmult\_accel.h}\label{1hC}
%\lstinputlisting[language=C]{code/mmult_accel.h}


\end{appendices}




\end{enumerate}
\end{document}






